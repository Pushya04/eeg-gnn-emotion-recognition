import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import ChebConv, global_mean_pool
from torch_geometric.utils import dense_to_sparse
from scipy.signal import hilbert
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.preprocessing import MinMaxScaler
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
import argparse

# Define EEG channel names (32 channels from DEAP dataset)
channels = ['Fp1', 'AF3', 'F3', 'F7', 'FC5', 'FC1', 'C3', 'T7', 'CP5', 'CP1', 'P3', 'P7', 'PO3',
            'O1', 'Oz', 'Pz', 'Fp2', 'AF4', 'Fz', 'F4', 'F8', 'FC6', 'FC2', 'Cz', 'C4', 'T8',
            'CP6', 'CP2', 'P4', 'P8', 'PO4', 'O2']

# Load EEG data from CSV file (exported for use in other scripts)
def load_data(subject_path, subject, label_type='valence'):
    """
    Load EEG features and labels from a CSV file generated by preprocessing.py.
    """
    file_path = os.path.join(subject_path, f'{subject}_features.csv')
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found at: {file_path}")
    df = pd.read_csv(file_path)
    features = df.drop(columns=['valence', 'arousal', 'four_class']).values  # 128 features (32 channels x 4 bands)
    labels = df[label_type].values  # Binary labels (0 or 1)
    return features, labels

# Compute Phase-Locking Value (PLV) adjacency matrix
def compute_plv(features):
    """
    Compute PLV-based adjacency matrix for EEG channels.
    """
    n_samples, n_features = features.shape
    n_channels = n_features // 4  # 32 channels, 4 bands each
    plv_matrix = np.zeros((n_channels, n_channels))
    
    # Extract frequency bands for each channel
    band_features = []
    for i in range(0, n_features, 4):
        channel_features = features[:, i:i+4]
        band_features.append(np.mean(channel_features, axis=1))  # Average across bands
    band_features = np.array(band_features)
    
    # Compute PLV for each pair of channels
    phases = np.angle(hilbert(band_features))
    for i in range(n_channels):
        for j in range(n_channels):
            if i != j:
                phase_diff = phases[i, :] - phases[j, :]
                plv_matrix[i, j] = np.abs(np.mean(np.exp(1j * phase_diff)))
            else:
                plv_matrix[i, j] = 1  # Self-connection
    return plv_matrix

# Compute Graph Laplacian and Graph Fourier Transform (GFT)
def compute_graph_fourier(adjacency_matrix):
    """
    Compute the Graph Laplacian and its eigendecomposition for GFT.
    Returns eigenvalues and eigenvectors for spectral analysis.
    """
    G = nx.from_numpy_array(adjacency_matrix)
    L = nx.laplacian_matrix(G).toarray()  # Unnormalized Laplacian: L = D - A
    eigenvalues, eigenvectors = np.linalg.eigh(L)  # Eigendecomposition
    return eigenvalues, eigenvectors

# Prepare PyTorch Geometric Data objects (exported for use in other scripts)
def prepare_data(features, labels, adjacency_matrix):
    """
    Convert features, labels, and adjacency matrix into PyTorch Geometric Data objects.
    """
    edge_index, edge_attr = dense_to_sparse(torch.tensor(adjacency_matrix, dtype=torch.float))
    data_list = []
    
    for i in range(features.shape[0]):
        n_channels = adjacency_matrix.shape[0]  # 32 channels
        n_bands = 4  # Theta, alpha, beta, gamma
        x = features[i].reshape(n_channels, n_bands)  # Node features: [32, 4]
        x = torch.tensor(x, dtype=torch.float)
        y = torch.tensor(labels[i], dtype=torch.long)
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
        data_list.append(data)
    return data_list

# Visualize the graph
def save_and_visualize_graph(adjacency_matrix, subject_path, label_type):
    """
    Visualize and save the EEG connectivity graph.
    """
    G = nx.from_numpy_array(adjacency_matrix)
    pos = nx.spring_layout(G, seed=42)
    
    plt.figure(figsize=(10, 8))
    
    # Color nodes by brain region
    frontal_nodes = [i for i, ch in enumerate(channels) if any(x in ch for x in ['F', 'Fp'])]
    central_nodes = [i for i, ch in enumerate(channels) if any(x in ch for x in ['C', 'T'])]
    parietal_nodes = [i for i, ch in enumerate(channels) if any(x in ch for x in ['P', 'CP'])]
    occipital_nodes = [i for i, ch in enumerate(channels) if any(x in ch for x in ['O', 'PO'])]
    
    nx.draw_networkx_nodes(G, pos, nodelist=frontal_nodes, node_color='lightblue', node_size=300, alpha=0.8)
    nx.draw_networkx_nodes(G, pos, nodelist=central_nodes, node_color='lightgreen', node_size=300, alpha=0.8)
    nx.draw_networkx_nodes(G, pos, nodelist=parietal_nodes, node_color='salmon', node_size=300, alpha=0.8)
    nx.draw_networkx_nodes(G, pos, nodelist=occipital_nodes, node_color='violet', node_size=300, alpha=0.8)
    
    edge_weights = [adjacency_matrix[u, v] * 2 for u, v in G.edges()]
    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')
    
    nx.draw_networkx_labels(G, pos, labels={i: channels[i] for i in range(len(channels))}, font_size=8)
    
    plt.title(f"EEG Functional Connectivity Graph (PLV) - {label_type.capitalize()}", fontsize=14)
    plt.axis('off')
    plt.tight_layout()
    
    graph_path = os.path.join(subject_path, f'{label_type}_graph_visualization.png')
    plt.savefig(graph_path, dpi=300)
    plt.close()
    return graph_path

# Define the Spectral GNN model with Chebyshev convolutions and GFT integration
class SpectralGNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, K=3):
        """
        Spectral GNN with Chebyshev convolutions.
        - input_dim: Number of input features per node (4 frequency bands)
        - hidden_dim: Hidden layer dimension
        - output_dim: Number of output classes (2 for binary classification)
        - K: Chebyshev polynomial order
        """
        super(SpectralGNN, self).__init__()
        self.cheb1 = ChebConv(input_dim, hidden_dim, K=K)  # First Chebyshev layer
        self.cheb2 = ChebConv(hidden_dim, hidden_dim, K=K)  # Second Chebyshev layer
        self.fc = nn.Linear(hidden_dim, output_dim)  # Classification layer
        self.dropout = nn.Dropout(0.6)  # Regularization

    def forward(self, x, edge_index, edge_attr, batch):
        """
        Forward pass with Chebyshev convolutions.
        """
        x = self.cheb1(x, edge_index)  # Spectral convolution
        x = F.relu(x)
        x = self.dropout(x)
        x = self.cheb2(x, edge_index)  # Second spectral convolution
        x = F.relu(x)
        x = global_mean_pool(x, batch)  # Graph-level pooling
        x = self.fc(x)  # Classification
        return x

# Training function
def train(model, loader, criterion, optimizer, num_epochs=100, subject_path=None, label_type=None):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.train()
    
    train_losses = []
    
    for epoch in range(num_epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for data in loader:
            data = data.to(device)
            optimizer.zero_grad()
            out = model(data.x, data.edge_index, data.edge_attr, data.batch)
            loss = criterion(out, data.y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pred = out.argmax(dim=1)
            correct += (pred == data.y).sum().item()
            total += data.y.size(0)
        
        avg_loss = total_loss / len(loader)
        accuracy = correct / total
        train_losses.append(avg_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')
    
    # Save training loss plot if paths provided
    if subject_path and label_type:
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(f'Training Loss for {label_type.capitalize()}')
        plt.grid(True)
        plt.savefig(os.path.join(subject_path, f'{label_type}_training_loss.png'))
        plt.close()
    
    return train_losses, model

# Evaluation function
def evaluate(model, loader, subject_path, subject, label_type):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    y_true, y_pred = [], []
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            out = model(data.x, data.edge_index, data.edge_attr, data.batch)
            pred = out.argmax(dim=1)
            y_true.extend(data.y.cpu().tolist())
            y_pred.extend(pred.cpu().tolist())
    
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted')
    print(f'{label_type.capitalize()} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}')
    
    # Save predictions
    df = pd.DataFrame({'True': y_true, 'Predicted': y_pred})
    df.to_csv(os.path.join(subject_path, f'{subject}_{label_type}_predictions.csv'), index=False)
    
    # Save confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix for {label_type.capitalize()}')
    plt.savefig(os.path.join(subject_path, f'{label_type}_confusion_matrix.png'))
    plt.close()
    
    return accuracy, f1

# Main execution function
def main(args):
    subject_path = os.path.join(args.datafiles_path, args.subject)
    os.makedirs(subject_path, exist_ok=True)
    
    # Set random seed for reproducibility
    torch.manual_seed(args.random_seed)
    np.random.seed(args.random_seed)
    
    # Load and preprocess data
    features, labels = load_data(subject_path, args.subject, args.label_type)
    scaler = MinMaxScaler()
    features = scaler.fit_transform(features)
    
    # Compute PLV-based adjacency matrix
    adjacency_matrix = compute_plv(features)
    adjacency_matrix[adjacency_matrix < args.plv_threshold] = 0
    
    # Compute Graph Fourier Transform (for analysis or visualization)
    eigenvalues, eigenvectors = compute_graph_fourier(adjacency_matrix)
    print(f"Graph Laplacian Eigenvalues (first 5): {eigenvalues[:5]}")
    
    # Visualize the graph
    save_and_visualize_graph(adjacency_matrix, subject_path, args.label_type)
    
    # Prepare data for PyTorch Geometric
    data_list = prepare_data(features, labels, adjacency_matrix)
    train_data, test_data = train_test_split(
        data_list, test_size=args.test_size, random_state=args.random_seed,
        stratify=[d.y.item() for d in data_list]
    )
    
    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)
    
    # Create directory for results
    label_dir = os.path.join(subject_path, args.label_type)
    os.makedirs(label_dir, exist_ok=True)
    
    # Initialize model, loss function, and optimizer
    model = SpectralGNN(
        input_dim=args.input_dim,
        hidden_dim=args.hidden_dim,
        output_dim=args.output_dim,
        K=args.cheb_k  # Chebyshev polynomial order
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # Train the model
    train_losses, model = train(
        model, train_loader, criterion, optimizer,
        num_epochs=args.num_epochs,
        subject_path=label_dir,
        label_type=args.label_type
    )
    
    # Evaluate the model
    accuracy, f1 = evaluate(model, test_loader, label_dir, args.subject, args.label_type)
    
    # Save model
    torch.save(model.state_dict(), os.path.join(label_dir, f'{args.subject}_{args.label_type}_model.pth'))
    
    # Save metrics
    metrics = {
        'subject': args.subject,
        'label_type': args.label_type,
        'accuracy': accuracy,
        'f1_score': f1,
        'num_epochs': args.num_epochs,
        'hidden_dim': args.hidden_dim,
        'learning_rate': args.learning_rate,
        'weight_decay': args.weight_decay,
        'plv_threshold': args.plv_threshold,
        'cheb_k': args.cheb_k
    }
    
    with open(os.path.join(label_dir, 'metrics.txt'), 'w') as f:
        for key, value in metrics.items():
            f.write(f'{key}: {value}\n')
    
    print(f"All results saved to {label_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Spectral GNN with GFT and Chebyshev Convolutions for EEG Emotion Recognition')
    
    # Data parameters
    parser.add_argument('--subject', type=str, default='s01', help='Subject ID')
    parser.add_argument('--datafiles_path', type=str, default='./output_files', help='Path to datafiles')
    parser.add_argument('--label_type', type=str, choices=['valence', 'arousal'], default='valence',
                        help='Label type for classification')
    
    # Model parameters
    parser.add_argument('--input_dim', type=int, default=4, help='Input dimension (4 frequency bands)')
    parser.add_argument('--hidden_dim', type=int, default=64, help='Hidden dimension')
    parser.add_argument('--output_dim', type=int, default=2, help='Output dimension (binary classification)')
    parser.add_argument('--cheb_k', type=int, default=3, help='Chebyshev polynomial order')
    
    # Training parameters
    parser.add_argument('--num_epochs', type=int, default=100, help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=0.005, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--test_size', type=float, default=0.3, help='Test set ratio')
    parser.add_argument('--random_seed', type=int, default=42, help='Random seed')
    
    # Graph parameters
    parser.add_argument('--plv_threshold', type=float, default=0.5, help='PLV threshold')
    
    args = parser.parse_args()
    main(args)